# main.py
# ============================================
# RAGçµ±åˆç‰ˆï¼ˆLlamaIndex + JSON FAQãƒŠãƒ¬ãƒƒã‚¸ï¼‰
# äº‹å‰æº–å‚™:
#   pip install "llama-index>=0.10.0" "llama-index-embeddings-huggingface>=0.2.0" "sentence-transformers>=2.6.0"
#   ï¼ˆHuggingFaceã®embeddingsã‚’åˆ©ç”¨ã—ã¾ã™ã€‚GPUã¯ä¸è¦ã€‚ï¼‰
# ç’°å¢ƒå¤‰æ•°ï¼ˆä»»æ„ï¼‰:
#   KNOWLEDGE_JSON: å‚ç…§ã™ã‚‹FAQ JSONãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ./faqs.jsonï¼‰
#   LMSTUDIO_URL:   LM Studioã®OpenAIäº’æ›ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆï¼ˆä¾‹: http://127.0.0.1:1234/v1ï¼‰
#   LMSTUDIO_MODEL: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«åï¼ˆä¾‹: openai/gpt-oss-120bï¼‰
#   MYCOEIROINK_URL: MyCoeiroInk TTSã®ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆï¼ˆä¾‹: http://127.0.0.1:50032ï¼‰
# ============================================

import os
import json
import base64
import requests
from typing import Optional, List, Dict

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import Response, HTMLResponse
from pydantic import BaseModel
import uvicorn

from openai import OpenAI

# === LlamaIndex (RAG) ========================
from llama_index.core import VectorStoreIndex, Document, Settings
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
# ============================================

# === è¨­å®š ========================
speakerUuid = "58adbc32-a00a-11f0-ac61-7e5b44f22354"  # MYCOEIROINKã®speakerinfoã§ç¢ºèª
styleId = 1043917874  # è©²å½“UUIDãƒ•ã‚©ãƒ«ãƒ€ã®meta.jsonãƒ•ã‚¡ã‚¤ãƒ«ã«è¨˜è¼‰

client = OpenAI(
    base_url=os.getenv("LMSTUDIO_URL", "http://127.0.0.1:1234/v1"),
    api_key="dummy_api_key"
)
model_name = os.getenv("LMSTUDIO_MODEL", "openai/gpt-oss-20b")

TTS_BASE_URL = os.getenv("MYCOEIROINK_URL", "http://127.0.0.1:50032")
KNOWLEDGE_JSON = os.getenv("KNOWLEDGE_JSON", "faqs.json")  # å…ˆã»ã©å‡ºåŠ›ã•ã›ãŸQAï¼ˆJSONï¼‰ã¸ã®ãƒ‘ã‚¹
#==================================

# === ã‚°ãƒ­ãƒ¼ãƒãƒ«ï¼ˆRAGï¼‰ =======================
RAG_INDEX: Optional[VectorStoreIndex] = None
RAG_TOP_K_DEFAULT = 3
RAG_SNIPPET_CHARS = int(os.getenv("RAG_SNIPPET_CHARS", "360"))
# ============================================

# å…ˆé ­ã®è¨­å®šä»˜è¿‘ã«è¿½åŠ ï¼ˆç’°å¢ƒå¤‰æ•°ã§èª¿æ•´å¯ï¼‰
MAX_TOKENS = int(os.getenv("MAX_TOKENS", "320"))    # å‡ºåŠ›ä¸Šé™
def getenv_float(name: str, default: float) -> float:
    v = os.getenv(name, None)
    if v is None:
        return default
    try:
        return float(v)
    except Exception:
        print(f"[WARN] Env {name}='{v}' is not a float. Using default={default}.")
        return default

LLM_TEMPERATURE = getenv_float("LLM_TEMPERATURE", 0.2)  # è¡çªã—ãªã„åå‰ã«å¤‰æ›´
CLIENT_TIMEOUT = int(os.getenv("CLIENT_TIMEOUT", "600"))  # 120â†’600 ã«å»¶é•·

messages = [
    {
        "role": "system",
        "content": (
            "ã‚ãªãŸã¯ç±³å­å·¥æ¥­é«˜ç­‰å°‚é–€å­¦æ ¡ã®ç·åˆå·¥å­¦ç§‘ãƒ»é›»æ°—é›»å­ã‚³ãƒ¼ã‚¹ã«æ‰€å±ã™ã‚‹5å¹´ç”Ÿã§ã™ã€‚"
            "åå‰ã¯äº•æ±ä½³å¸Œ(ã„ã¨ã†ã‚ˆã—ã)ã§ã™ã€‚ã‚ãªãŸã®è¶£å‘³ã¯ç¥ç¤¾ä»é–£å·¡ã‚Šã§ã€å¥½ããªå¯ºç¤¾ã¯æ°¸å¹³å¯ºã€å››å¤©ç‹å¯ºã§ã™ã€‚"
            "ã‚ãªãŸã¯2å¹´ç”Ÿã®ã“ã‚ã¾ã§æ”¾é€éƒ¨ã«æ‰€å±ã—ã¦ãŠã‚Šã€ãƒ‰ãƒ©ãƒã‚„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’åˆ¶ä½œã—ã¦ã„ã¾ã—ãŸãŒç¾åœ¨ã¯é€€éƒ¨ã—ã€"
            "ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿åŒå¥½ä¼šã¨æ•°å­¦åŒå¥½ä¼šã«æ‰€å±ã—ã¦ã„ã¾ã™ã€‚"
        )
    }
]

app = FastAPI()
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=False,
    allow_methods=["*"],
    allow_headers=["*"],
    expose_headers=["X-LLM-Text-B64"],
)

# ========= RAG: JSON -> Index æ§‹ç¯‰ ===========
def build_rag_index_from_json(json_path: str) -> Optional[VectorStoreIndex]:
    """
    JSONå½¢å¼ã®FAQãƒŠãƒ¬ãƒƒã‚¸ã‚’èª­ã¿è¾¼ã¿ã€LlamaIndexã®VectorStoreIndexã‚’æ§‹ç¯‰ã™ã‚‹ã€‚
    æœŸå¾…ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ:
      {
        "faqs": [
          {"question": "...", "answer": "..."},
          ...
        ]
      }
    """
    if not os.path.exists(json_path):
        print(f"[RAG] knowledge json not found: {json_path}")
        return None

    try:
        with open(json_path, "r", encoding="utf-8") as f:
            data = json.load(f)
    except Exception as e:
        print(f"[RAG] json load error: {e}")
        return None

    faqs: List[Dict[str, str]] = data.get("faqs", [])
    if not faqs:
        print("[RAG] faqs list empty")
        return None

    # Embeddingãƒ¢ãƒ‡ãƒ«è¨­å®šï¼ˆHuggingFaceï¼‰
    # è»½é‡ã‹ã¤ç²¾åº¦ãƒãƒ©ãƒ³ã‚¹ã®è‰¯ã„ all-MiniLM-L6-v2 ã‚’ä½¿ç”¨
    embed_model = HuggingFaceEmbedding(model_name="sentence-transformers/all-MiniLM-L6-v2")
    Settings.embed_model = embed_model

    documents: List[Document] = []
    for i, item in enumerate(faqs, start=1):
        q = item.get("question", "").strip()
        a = item.get("answer", "").strip()
        # æ¤œç´¢å¯¾è±¡ã®æœ¬æ–‡ï¼ˆQã¨Aã‚’ä½µè¨˜ï¼‰
        body = f"Q: {q}\nA: {a}"
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼ˆå¾Œã§å‡ºå…¸è¡¨è¨˜ãªã©ã«ã‚‚ä½¿ãˆã‚‹ï¼‰
        meta = {"source": os.path.basename(json_path), "faq_id": i, "question": q}
        documents.append(Document(text=body, metadata=meta))

    index = VectorStoreIndex.from_documents(documents)
    print(f"[RAG] index built with {len(documents)} docs from {json_path}")
    return index


def get_rag_context(query: str, top_k: int = RAG_TOP_K_DEFAULT) -> str:
    global RAG_INDEX
    if RAG_INDEX is None:
        return ""
    hits = RAG_INDEX.as_retriever(similarity_top_k=top_k).retrieve(query)
    lines = []
    for rank, hit in enumerate(hits, start=1):
        content = hit.node.get_content().strip()
        # Aã®è¦ç‚¹ã ã‘ã‚’æŠœããªã‚‰ã“ã“ã§ç°¡æ˜“æŠ½å‡ºã—ã¦ã‚‚OK
        snippet = content[:RAG_SNIPPET_CHARS]
        src = hit.node.metadata.get("source", "")
        fid = hit.node.metadata.get("faq_id", "")
        lines.append(f"[{rank}] (source={src}#{fid})\n{snippet}")
    return "\n\n".join(lines)


def make_rag_system_instruction(context_text: str) -> str:
    if not context_text:
        return ""
    return (
        "ä»¥ä¸‹ã¯ç±³å­é«˜å°‚(ç±³å­å·¥æ¥­é«˜ç­‰å°‚é–€å­¦æ ¡)ã®ç†å¿µãƒ»åˆ¶åº¦ãƒ»å­¦å†…è¦ç¨‹ãƒ»é‹ç”¨ã«é–¢ã™ã‚‹ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ï¼ˆFAQ JSONï¼‰ã‹ã‚‰ã®æ¤œç´¢çµæœã§ã™ã€‚\n"
        "ã‚ãªãŸã®å›ç­”ã¯ã€ã¾ãšã“ã®å‚ç…§æƒ…å ±ã«å³å¯†ã«åŸºã¥ãã€äº‹å®Ÿã«å¿ å®Ÿã«è¦ç‚¹ã‚’ã¾ã¨ã‚ã¦æ–‡ç« ã«ã—ã¦ãã ã•ã„ã€‚\n"
        "ä¸æ˜ç‚¹ãŒã‚ã‚‹å ´åˆã¯ã€æ¨æ¸¬ã›ãšã«ã€ãã®è³ªå•ã¯ã‚ã‹ã‚‰ã‚“ã€ã¨è¿°ã¹ãŸä¸Šã§ã€é–¢é€£ã—ãã†ãªæƒ…å ±ã‚’å‚ç…§æƒ…å ±ã‹ã‚‰è£œè¶³ã—ã¦ãã ã•ã„ã€‚\n"
        "ã‚‚ã—å‚ç…§æƒ…å ±ãŒé•·ã™ãã‚‹å ´åˆã«ã¯2ã€3æ–‡ç¨‹åº¦ã«è¦ç´„ã—ã¦æ–‡ç« ã¨ã—ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚"
        "-ç®‡æ¡æ›¸ãã®ã‚ˆã†ãªæ›¸ãæ–¹ã‚’çµ¶å¯¾ã«å›ç­”ã«å«ã‚ãªã„ã§ãã ã•ã„ã€‚\n"
        "ã€Œå‚ç…§æƒ…å ±ã«ã€ã®ã‚ˆã†ãªæ›¸ãæ–¹ã‚’çµ¶å¯¾ã«å›ç­”ã«å«ã‚ãªã„ã§ãã ã•ã„ã€‚"
        "- ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã«ç„¡ã„æƒ…å ±ã¯çµ¶å¯¾ã«å›ç­”ã«å«ã‚ãªã„ã§ãã ã•ã„ã€‚\n"
        "- ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã®å†…å®¹ã‚’ä¸¸æŠ•ã’ã™ã‚‹ã®ã§ã¯ãªãã€çµ¶å¯¾ã«ãã¡ã‚“ã¨ã—ãŸæ–‡ç« ã«ã—ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚\n"
        "- è³ªå•ã®ç­”ãˆã‚’çŸ¥ã‚‰ãªã„å ´åˆã¯ã€èª¤ã£ãŸæƒ…å ±ã‚’å…±æœ‰ã—ãªã„ã§ãã ã•ã„ã€‚\n"
        f"ã€å‚ç…§æƒ…å ±ï¼ˆä¸Šä½å€™è£œï¼‰ã€‘\n{context_text}\n"
    )
# ============================================


class ChatIn(BaseModel):
    message: str
    system_prompt: Optional[str] = None


def call_llm_with(user_text: str, system_prompt: Optional[str] = None) -> str:
    # ã‚¹ãƒ†ãƒ¼ãƒˆãƒ¬ã‚¹ï¼ˆæ¯å›æœ€å°æ§‹æˆï¼‰
    convo = []

    # ç°¡ç•¥åŒ–ã—ãŸãƒšãƒ«ã‚½ãƒŠï¼ˆ1ã€œ2æ–‡ï¼‰
    base_persona = "ã‚ãªãŸã¯ç±³å­å·¥æ¥­é«˜ç­‰å°‚é–€å­¦æ ¡ã®å­¦ç”Ÿã¨ã—ã¦ã€ç°¡æ½”ã‹ã¤äº‹å®Ÿã«åŸºã¥ãå›ç­”ã—ã¾ã™ã€‚"
    convo.append({"role": "system", "content": base_persona})

    # RAG
    rag_context = get_rag_context(user_text, top_k=3)  # 5â†’3
    rag_sys = make_rag_system_instruction(rag_context)
    if rag_sys:
        convo.insert(0, {"role": "system", "content": rag_sys})

    if system_prompt:
        convo.insert(0, {"role": "system", "content": system_prompt})

    convo.append({"role": "user", "content": user_text})

    resp = client.chat.completions.create(
    model=model_name,
    messages=convo,
    timeout=CLIENT_TIMEOUT,      # æ—¢å­˜
    max_tokens=MAX_TOKENS,       # æ—¢å­˜
    temperature=LLM_TEMPERATURE, # ã“ã“ã‚’å¤‰æ›´
    )
    reply = (resp.choices[0].message.content or "").strip()
    return reply

def generate_my_voice(text: str) -> bytes:
    query = {
        "speakerUuid": speakerUuid,
        "styleId": styleId,
        "text": text,
        "speedScale": 1.0,
        "volumeScale": 1.0,
        "prosodyDetail": [],
        "pitchScale": 0.0,
        "intonationScale": 1.0,
        "prePhonemeLength": 0.1,
        "postPhonemeLength": 0.5,
        "outputSamplingRate": 24000,
    }
    r = requests.post(f"{TTS_BASE_URL}/v1/synthesis", json=query, timeout=60)
    if r.status_code >= 400:
        print("TTS /v1/synthesis error:", r.status_code, r.text[:1000])
    r.raise_for_status()
    return r.content


# WAV ã‚’ç›´æ¥è¿”ã™APIï¼ˆãƒ†ã‚­ã‚¹ãƒˆã¯ base64 ã§ãƒ˜ãƒƒãƒ€ã«å…¥ã‚Œã‚‹ï¼‰
@app.post("/chat_tts_wav")
def chat_tts_wav(inp: ChatIn):
    answer = call_llm_with(inp.message, system_prompt=inp.system_prompt)
    wav_bytes = generate_my_voice(answer)
    answer_b64 = base64.b64encode(answer.encode("utf-8")).decode("ascii")  # ASCII ã®ã¿
    headers = {"X-LLM-Text-B64": answer_b64}
    return Response(content=wav_bytes, media_type="audio/wav", headers=headers)


# ï¼ˆä»»æ„ï¼‰ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å—ã‘å–ã‚Šã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆï¼ˆUIã‹ã‚‰å‘¼ã°ã‚Œã‚‹ï¼‰
class FeedbackIn(BaseModel):
    user_message: str
    assistant_text: str
    rating: str
    comment: str = ""


@app.post("/feedback")
def feedback(inp: FeedbackIn):
    # å¿…è¦ã«å¿œã˜ã¦ãƒ­ã‚°ä¿å­˜ã‚„å­¦ç¿’ç”¨è“„ç©ã‚’å®Ÿè£…
    print(f"[FB] rating={inp.rating} user='{inp.user_message[:60]}' asst='{inp.assistant_text[:60]}'")
    return {"status": "ok"}


# RAGã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®å†èª­è¾¼APIï¼ˆãƒŠãƒ¬ãƒƒã‚¸å·®ã—æ›¿ãˆæ™‚ã«ä½¿ç”¨ï¼‰
@app.post("/reload_rag")
def reload_rag():
    global RAG_INDEX
    RAG_INDEX = build_rag_index_from_json(KNOWLEDGE_JSON)
    return {"status": "ok", "index_ready": RAG_INDEX is not None}


# ã‚¹ãƒãƒ›ã§ã‚‚ä½¿ãˆã‚‹æœ€å°UI
@app.get("/", response_class=HTMLResponse)
def index():
    return HTMLResponse("""
<!doctype html>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<style>
  body { font-family: system-ui, sans-serif; margin: 1rem; }
  #chat { border: 1px solid #ccc; border-radius: 8px; padding: .5rem; max-height: 60vh; overflow:auto; }
  .msg { margin: .5rem 0; }
  .user { color: #0b5; }
  .assistant { color: #06c; }
  .row { display:flex; gap:.5rem; margin-top:.5rem; }
  button { padding:.5rem 1rem; }
  #player { width:100%; margin-top:.5rem; }
</style>
<h3>Chat + TTS (RAG enabled)</h3>
<div id="chat"></div>
<div class="row">
  <input id="msg" placeholder="ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›" style="flex:1; padding:.5rem;" />
  <button id="send">é€ä¿¡</button>
</div>
<audio id="player" controls></audio>
<div id="fb" style="display:none; margin-top:.5rem;">
  ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯:
  <button id="up">ğŸ‘</button>
  <button id="down">ğŸ‘</button>
</div>
<script>
  const chat = document.getElementById('chat');
  const msg = document.getElementById('msg');
  const send = document.getElementById('send');
  const player = document.getElementById('player');
  const fb = document.getElementById('fb');
  const up = document.getElementById('up');
  const down = document.getElementById('down');

  let lastUser = "";
  let lastAssistant = "";

  function addMsg(text, cls) {
    const div = document.createElement('div');
    div.className = 'msg ' + cls;
    div.textContent = (cls === 'user' ? 'ã‚ãªãŸ: ' : 'ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ: ') + text;
    chat.appendChild(div);
    chat.scrollTop = chat.scrollHeight;
  }

  function b64ToUtf8(b64) {
    const bin = atob(b64);
    const bytes = new Uint8Array(bin.length);
    for (let i = 0; i < bin.length; i++) bytes[i] = bin.charCodeAt(i);
    return new TextDecoder().decode(bytes);
  }

  async function sendChat() {
    const text = msg.value.trim();
    if (!text) return;
    msg.value = "";
    addMsg(text, 'user');
    lastUser = text;

    const res = await fetch('/chat_tts_wav', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ message: text })
    });

    const b64 = res.headers.get('X-LLM-Text-B64') || '';
    const answer = b64 ? b64ToUtf8(b64) : '';
    lastAssistant = answer;
    addMsg(answer, 'assistant');

    const blob = await res.blob();
    player.src = URL.createObjectURL(blob);
    try { await player.play(); } catch(e) {}
    fb.style.display = 'block';
  }

  send.onclick = sendChat;
  msg.addEventListener('keydown', (e) => { if (e.key === 'Enter') sendChat(); });

  async function sendFeedback(rating) {
    if (!lastAssistant) return;
    await fetch('/feedback', {
      method: 'POST',
      headers: {'Content-Type':'application/json'},
      body: JSON.stringify({
        user_message: lastUser,
        assistant_text: lastAssistant,
        rating, comment: ""
      })
    });
    alert('ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’é€ä¿¡ã—ã¾ã—ãŸã€‚ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ã€‚');
  }
  up.onclick = () => sendFeedback('up');
  down.onclick = () => sendFeedback('down');
</script>
    """)

# ===========================
@app.on_event("startup")
def _startup():
    # èµ·å‹•æ™‚ã«RAGã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰
    global RAG_INDEX
    RAG_INDEX = build_rag_index_from_json(KNOWLEDGE_JSON)
    if RAG_INDEX is None:
        print("[RAG] WARN: RAG index not ready. Set KNOWLEDGE_JSON or place faqs.json.")

if __name__ == "__main__":
    # ã‚¹ãƒãƒ›ã‹ã‚‰ã‚‚ä½¿ã†ãªã‚‰ host=0.0.0.0
    uvicorn.run(app, host="0.0.0.0", port=8000)